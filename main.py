#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ–± –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –º–∞–≥–∞–∑–∏–Ω–µ
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Ç–∏–ª–∏—Ç—ã –∏–∑ utils.py
"""

import json
import os
import pickle
import sys
from datetime import datetime
import utils
from openai import OpenAI
from utils import chunk_text, create_embeddings, generate_response

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI –∫–ª–∏–µ–Ω—Ç–∞ (–Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å API –∫–ª—é—á)
client = OpenAI(
    
    api_key=os.getenv("OPENAI_API_KEY", "your-api-key-here")
)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–ª–∏–µ–Ω—Ç –≤ –º–æ–¥—É–ª–µ utils –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
utils.client = client

def load_qa_data(filename):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            data = json.load(file)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
        return data
    except FileNotFoundError:
        print(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []
    except json.JSONDecodeError as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ JSON: {e}")
        return []

def check_vector_database_exists(filename="vector_db.pkl"):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π"""
    return os.path.exists(filename)

def save_vector_database(text_chunks, embeddings, filename="vector_db.pkl"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –≤ —Ñ–∞–π–ª"""
    try:
        vector_db = {
            'text_chunks': text_chunks,
            'embeddings': embeddings,
            'metadata': {
                'created_at': datetime.now(),
                'chunks_count': len(text_chunks),
                'embeddings_count': len(embeddings),
                'source_file': 'ecommerce_qa.json'
            }
        }
        
        with open(filename, 'wb') as f:
            pickle.dump(vector_db, f)
        
        print(f"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filename}")
        print(f"  - –ß–∞–Ω–∫–æ–≤: {len(text_chunks)}")
        print(f"  - –≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings)}")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")

def load_vector_database(filename="vector_db.pkl"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∏–∑ —Ñ–∞–π–ª–∞"""
    try:
        with open(filename, 'rb') as f:
            vector_db = pickle.load(f)
        
        text_chunks = vector_db['text_chunks']
        embeddings = vector_db['embeddings']
        metadata = vector_db.get('metadata', {})
        
        print(f"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filename}")
        print(f"  - –°–æ–∑–¥–∞–Ω–∞: {metadata.get('created_at', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        print(f"  - –ß–∞–Ω–∫–æ–≤: {metadata.get('chunks_count', len(text_chunks))}")
        print(f"  - –≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {metadata.get('embeddings_count', len(embeddings))}")
        
        return text_chunks, embeddings
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")
        return [], []

def prepare_knowledge_base(qa_data, chunk_size=500, overlap=50):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ –æ—Ç–≤–µ—Ç–æ–≤"""
    print("–°–æ–∑–¥–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –æ—Ç–≤–µ—Ç—ã –≤ –æ–¥–Ω—É –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    all_answers = []
    for item in qa_data:
        answer = item.get('answer', '')
        if answer:
            all_answers.append(answer)
    
    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –≤—Å–µ—Ö –æ—Ç–≤–µ—Ç–æ–≤
    all_chunks = []
    for answer in all_answers:
        chunks = chunk_text(answer, chunk_size, overlap)
        all_chunks.extend(chunks)
    
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
    print("–°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    embeddings = []
    for i, chunk in enumerate(all_chunks):
        if i % 10 == 0:
            print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(all_chunks)} —á–∞–Ω–∫–æ–≤")
        
        try:
            embedding_response = create_embeddings(chunk, client=client)
            embeddings.append(embedding_response.data[0])
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {i}: {e}")
            continue
    
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    return all_chunks, embeddings

def semantic_search_wrapper(query, text_chunks, embeddings, k=3):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    try:
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding_response = create_embeddings(query, client=client)
        query_embedding = query_embedding_response.data[0].embedding
        
        similarity_scores = []
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∫–∞–∂–¥—ã–º —á–∞–Ω–∫–æ–º
        for i, chunk_embedding in enumerate(embeddings):
            # –ü—Ä–æ—Å—Ç–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
            import numpy as np
            
            vec1 = np.array(query_embedding)
            vec2 = np.array(chunk_embedding.embedding)
            
            # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
            similarity_scores.append((i, similarity))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity_scores.sort(key=lambda x: x[1], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        top_indices = [index for index, _ in similarity_scores[:k]]
        return [text_chunks[index] for index in top_indices]
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø–æ–∏—Å–∫–µ: {e}")
        return []

def rag_answer(question, text_chunks, embeddings, max_context_length=1500):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è RAG"""
    print(f"\n–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å: {question}")
    
    # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
    relevant_chunks = semantic_search_wrapper(question, text_chunks, embeddings, k=3)
    
    if not relevant_chunks:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context = "\n\n".join(relevant_chunks)
    
    # –û–±—Ä–µ–∑–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
    if len(context) > max_context_length:
        context = context[:max_context_length] + "..."
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
    system_prompt = """–í—ã - –ø–æ–º–æ—â–Ω–∏–∫ —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –º–∞–≥–∞–∑–∏–Ω–∞. 
–û—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
–ë—É–¥—å—Ç–µ –≤–µ–∂–ª–∏–≤—ã, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã –∏ —Ç–æ—á–Ω—ã.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏—Ç–µ –æ–± —ç—Ç–æ–º."""
    
    user_message = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:
{context}

–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {question}

–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
    
    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = generate_response(system_prompt, user_message)
        return response.choices[0].message.content
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=== RAG –°–ò–°–¢–ï–ú–ê –î–õ–Ø –ò–ù–¢–ï–†–ù–ï–¢ –ú–ê–ì–ê–ó–ò–ù–ê ===")
    print("üí° –ü–æ–¥—Å–∫–∞–∑–∫–∞: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'python main.py --rebuild' –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    rebuild_database = "--rebuild" in sys.argv or "-r" in sys.argv
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    qa_data = load_qa_data("ecommerce_qa.json")
    if not qa_data:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
    if rebuild_database:
        print("\nüîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã...")
        text_chunks, embeddings = prepare_knowledge_base(qa_data)
        
        if text_chunks and embeddings:
            save_vector_database(text_chunks, embeddings)
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
            return
            
    elif check_vector_database_exists():
        print("\nüì¶ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö")
        text_chunks, embeddings = load_vector_database()
        
        if not text_chunks or not embeddings:
            print("‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –±–∞–∑—É...")
            text_chunks, embeddings = prepare_knowledge_base(qa_data)
            save_vector_database(text_chunks, embeddings)
    else:
        print("\nüîß –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é...")
        text_chunks, embeddings = prepare_knowledge_base(qa_data)
        
        if text_chunks and embeddings:
            save_vector_database(text_chunks, embeddings)
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
            return
    
    print(f"\n‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
    print(f"   üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(text_chunks)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤")
    print(f"   üîç –î–æ—Å—Ç—É–ø–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
    
    print("\n=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï RAG –°–ò–°–¢–ï–ú–´ ===")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö –∏–∑ JSON
    test_questions = [
        qa_data[0]["question"],  # –ü–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å
        qa_data[5]["question"],  # –®–µ—Å—Ç–æ–π –≤–æ–ø—Ä–æ—Å  
        qa_data[10]["question"], # –û–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç—ã–π –≤–æ–ø—Ä–æ—Å
        "–ö–∞–∫–æ–π —É –≤–∞—Å –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞?",  # –ù–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å
        "–ú–æ–∂–Ω–æ –ª–∏ –∫—É–ø–∏—Ç—å –≤ –∫—Ä–µ–¥–∏—Ç?"     # –ï—â–µ –æ–¥–∏–Ω –Ω–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n--- –¢–ï–°–¢ {i} ---")
        answer = rag_answer(question, text_chunks, embeddings)
        print(f"–í–û–ü–†–û–°: {question}")
        print(f"–û–¢–í–ï–¢ RAG: {answer}")
        print("-" * 80)
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print("\n=== –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú ===")
    print("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã (–∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):")
    
    while True:
        user_question = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
        
        if user_question.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', '']:
            print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        answer = rag_answer(user_question, text_chunks, embeddings)
        print(f"\n–û—Ç–≤–µ—Ç: {answer}")

if __name__ == "__main__":
    main()
